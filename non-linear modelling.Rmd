---
title: "Nuevos resultados paper 2 - Negativo"
subtitle: "Modelo RF - Árboles - XGB"
author: "Juliana Zapata"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      smooth_scroll: false
      collapsed: false
    code_folding: hide
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")
```

# Bibliotecas

```{r}
library(tidyverse)
library(janitor) 
library(readxl) 
library(DT)
library(tidymodels)
library(DALEX)
library(DALEXtra)
library(mgcv)
library(splines)

theme_set(theme_minimal())
```

# Datos

```{r}
df_indices <- read_excel("datos_indices_acp_paper2.xlsx") %>%
  rename(Positive_index = comp1) %>%
  rename(Negative_index = comp2) %>%
  rename(Neutral_index = comp3)

df_modelos <-
  df_indices %>%
  select(centroid:vertical_density,
         Positive_index,
         Negative_index,
         Neutral_index)


# modelo17 <- gam(Positive_index ~ ns(bpm, df = 2)
#                 + ns(vertical_density, df = 2),
#                 data = df_indices)
# 
# modelo30 <- gam(Negative_index ~ ns(bpm, df = 2)
#                 + hfc
#                 + ns(spectral_deviation, df = 2)
#                 + ns(vertical_density, df = 2)
#                 + ns(zcr, df = 2),
#                 data = df_indices)
# 
# modelo51 <- gam(Neutral_index ~ ns(bpm, df = 2)
#                 + ns(vertical_density, df = 2),
#                 data = df_indices)


```

# Índice negativo RF {.tabset .tabset-fade .tabset-pills}

## Entrenamiento 

```{r, eval=FALSE}
# Partición inicial
set.seed(2022)
my_split1 <-
  initial_split(data = df_modelos %>% select(-c(Positive_index, Neutral_index)),
                prop = 0.80,
                strata = Negative_index)

df_train1 <- training(my_split1)
df_test1 <- testing(my_split1)

# Validación cruzada con k = 10
set.seed(2022)
folds1 <- vfold_cv(data = df_train1,
                  prop = 0.80,
                  strata = Negative_index,
                  v = 10,
                  repeats = 5)

# Modelo
modelo_rf <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine(engine = "ranger", importance = "impurity") %>%
  set_mode(mode = "regression")

# Grid de hiperparámetros
set.seed(2022)
my_grid <- grid_max_entropy(mtry(range = c(2, 8)), trees(), min_n(),  size = 20)

# Receta
receta1 <-
  df_train1 %>%
  recipe(Negative_index ~ .)

# Flujo
flujo1 <-
  workflow() %>% 
  add_recipe(receta1) %>% 
  add_model(modelo_rf)

# Ajuste
control <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

doParallel::registerDoParallel()

set.seed(2022)
my_tuning <- tune_grid(flujo1,
                       resamples = folds1,
                       grid = my_grid,
                       control = control)

doParallel::stopImplicitCluster()
```

```{r, echo=FALSE}
#write_rds(x = my_tuning, file = "grid_rf_negativo.rds")

my_tuning <- read_rds("grid_rf_negativo.rds")

# Partición inicial
set.seed(2022)
my_split1 <-
  initial_split(data = df_modelos %>% select(-c(Positive_index, Neutral_index)),
                prop = 0.80,
                strata = Negative_index)

df_train1 <- training(my_split1)
df_test1 <- testing(my_split1)

# Validación cruzada con k = 10
set.seed(2022)
folds1 <- vfold_cv(data = df_train1,
                  prop = 0.80,
                  strata = Negative_index,
                  v = 10,
                  repeats = 5)

# Modelo
modelo_rf <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine(engine = "ranger", importance = "impurity") %>%
  set_mode(mode = "regression")

# Grid de hiperparámetros
set.seed(2022)
my_grid <- grid_max_entropy(mtry(range = c(2, 8)), trees(), min_n(),  size = 20)

# Receta
receta1 <-
  df_train1 %>%
  recipe(Negative_index ~ .)

# Flujo
flujo1 <-
  workflow() %>% 
  add_recipe(receta1) %>% 
  add_model(modelo_rf)
```


## Resultados modelos

- Mejor modelo según R cuadrado:

```{r}
my_tuning %>% 
  collect_metrics() %>% 
  filter(.metric == "rsq") %>% 
  arrange(desc(mean))
```

- Mejor modelo según RMSE:

```{r}
my_tuning %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

## Mejor modelo

```{r}
best_model <- my_tuning %>%
  select_best(metric = "rmse")

doParallel::registerDoParallel()

final_model <- flujo1 %>%
  finalize_workflow(best_model) %>%
  last_fit(my_split1)

doParallel::stopImplicitCluster()

final_model %>% 
  collect_metrics()
```


## Precichos vs Reales

```{r}
my_tuning %>% 
  collect_predictions() %>% 
  filter(.config == "Preprocessor1_Model07") %>% 
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

## Predichos vs Reales (Test)

```{r}
doParallel::registerDoParallel()

ajuste_rf <- flujo1 %>%
  finalize_workflow(best_model) %>%
  fit(df_train1)

doParallel::stopImplicitCluster()


tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_rf, new_data = df_test1)$.pred
) %>%
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

- Correlación de Spearman:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_rf, new_data = df_test1)$.pred
) %>% 
  cor(method = "spearman")
```


# Índice negativo AD {.tabset .tabset-fade .tabset-pills}

## Entrenamiento 

```{r, eval = FALSE}
# Modelo
modelo_ad <-
  decision_tree(
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "regression")

# Grid de hiperparámetros
set.seed(2022)
my_grid2 <-
  grid_max_entropy(tree_depth(), min_n(),  size = 20)

# Flujo
flujo2 <-
  workflow() %>%
  add_recipe(receta1) %>%
  add_model(modelo_ad)

# Ajuste
doParallel::registerDoParallel()

set.seed(2022)
my_tuning2 <- tune_grid(flujo2,
                        resamples = folds1,
                        grid = my_grid2,
                        control = control)

doParallel::stopImplicitCluster()
```

```{r, echo=FALSE}
#write_rds(x = my_tuning2, file = "grid_ad_negativo.rds")

my_tuning2 <- read_rds("grid_ad_negativo.rds")

# Modelo
modelo_ad <-
  decision_tree(
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "regression")

# Grid de hiperparámetros
set.seed(2022)
my_grid2 <-
  grid_max_entropy(tree_depth(), min_n(),  size = 20)

# Flujo
flujo2 <-
  workflow() %>%
  add_recipe(receta1) %>%
  add_model(modelo_ad)
```


## Resultados modelos

- Mejor modelo según R cuadrado:

```{r}
my_tuning2 %>% 
  collect_metrics() %>% 
  filter(.metric == "rsq") %>% 
  arrange(desc(mean))
```

- Mejor modelo según RMSE:

```{r}
my_tuning2 %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

## Mejor modelo

```{r}
best_model2 <- my_tuning2 %>%
  select_best(metric = "rmse")

doParallel::registerDoParallel()

final_model2 <- flujo2 %>%
  finalize_workflow(best_model2) %>%
  last_fit(my_split1)

doParallel::stopImplicitCluster()

final_model2 %>% 
  collect_metrics()
```


## Precichos vs Reales

```{r}
my_tuning2 %>% 
  collect_predictions() %>% 
  filter(.config == "Preprocessor1_Model02") %>% 
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

## Predichos vs Reales (Test)

```{r}
doParallel::registerDoParallel()

ajuste_ad <- flujo2 %>%
  finalize_workflow(best_model2) %>%
  fit(df_train1)

doParallel::stopImplicitCluster()


tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_ad, new_data = df_test1)$.pred
) %>%
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

- Correlación de Spearman:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_ad, new_data = df_test1)$.pred
) %>% 
  cor(method = "spearman")
```

# Índice negativo XGB {.tabset .tabset-fade .tabset-pills}

## Entrenamiento 

```{r, eval = FALSE}
# Modelo
modelo_xgboost <- boost_tree(
  tree_depth = tune(),
  trees = tune(),
  learn_rate = tune(),
  mtry = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = 100
) %>%
  set_mode(mode = "regression") %>%
  set_engine("xgboost")

# Grid de hiperparámetros
set.seed(2022)
my_grid3 <-
  grid_max_entropy(
    tree_depth(range = c(2L, 8L)),
    trees(),
    learn_rate(),
    mtry(range = c(2, 8)),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(range = c(0.5, 1)),
    size = 20
  )

# Flujo
flujo3 <-
  workflow() %>%
  add_recipe(receta1) %>%
  add_model(modelo_xgboost)

# Ajuste
doParallel::registerDoParallel()

set.seed(2022)
my_tuning3 <- tune_grid(flujo3,
                        resamples = folds1,
                        grid = my_grid3,
                        control = control)

doParallel::stopImplicitCluster()
```

```{r, echo=FALSE}
#write_rds(x = my_tuning3, file = "grid_xgb_negativo.rds")

my_tuning3 <- read_rds("grid_xgb_negativo.rds")

# Modelo
modelo_xgboost <- boost_tree(
  tree_depth = tune(),
  trees = tune(),
  learn_rate = tune(),
  mtry = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  stop_iter = 100
) %>%
  set_mode(mode = "regression") %>%
  set_engine("xgboost")

# Grid de hiperparámetros
set.seed(2022)
my_grid3 <-
  grid_max_entropy(
    tree_depth(range = c(2L, 8L)),
    trees(),
    learn_rate(),
    mtry(range = c(2, 8)),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(range = c(0.5, 1)),
    size = 20
  )

# Flujo
flujo3 <-
  workflow() %>%
  add_recipe(receta1) %>%
  add_model(modelo_xgboost)
```


## Resultados modelos

- Mejor modelo según R cuadrado:

```{r}
my_tuning3 %>% 
  collect_metrics() %>% 
  filter(.metric == "rsq") %>% 
  arrange(desc(mean))
```

- Mejor modelo según RMSE:

```{r}
my_tuning3 %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)
```

## Mejor modelo

```{r}
best_model3 <- my_tuning3 %>%
  select_best(metric = "rmse")

doParallel::registerDoParallel()

final_model3 <- flujo3 %>%
  finalize_workflow(best_model3) %>%
  last_fit(my_split1)

doParallel::stopImplicitCluster()

final_model3 %>% 
  collect_metrics()
```


## Precichos vs Reales

```{r}
my_tuning3 %>% 
  collect_predictions() %>% 
  filter(.config == "Preprocessor1_Model09") %>% 
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

## Predichos vs Reales (Test)

```{r}
doParallel::registerDoParallel()

ajuste_xgb <- flujo3 %>%
  finalize_workflow(best_model3) %>%
  fit(df_train1)

doParallel::stopImplicitCluster()


tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_xgb, new_data = df_test1)$.pred
) %>%
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

- Correlación de Spearman:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(ajuste_xgb, new_data = df_test1)$.pred
) %>% 
  cor(method = "spearman")
```

# Índice negativo GAM {.tabset .tabset-fade .tabset-pills}

## Entrenamiento

```{r}
modelo_gam <- gam(
  Negative_index ~ ns(bpm, df = 2)
  + hfc
  + ns(spectral_deviation, df = 2)
  + ns(vertical_density, df = 2)
  + ns(zcr, df = 2),
  data = df_train1
)
```

## Resultados modelos

```{r}
anova(modelo_gam)
```

## Predichos vs Reales (Test)

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(modelo_gam, newdata = df_test1)
) %>%
  ggplot(aes(x = Negative_index, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

- Correlación de Spearman:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(modelo_gam, newdata = df_test1)
) %>% 
  cor(method = "spearman")
```

## RMSE y R2

- RMSE:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(modelo_gam, newdata = df_test1)
)  %>% 
  rmse(truth = Negative_index, estimate = .pred)
```

- R2:

```{r}
tibble(
  Negative_index = df_test1$Negative_index,
  .pred = predict(modelo_gam, newdata = df_test1)
)  %>% 
  rsq(truth = Negative_index, estimate = .pred)
```

# Explicación de modelos {.tabset .tabset-fade .tabset-pills}

## Random Forest {.tabset .tabset-fade .tabset-pills}

### Predicción nueva

```{r}
df_explicacion <-
  df_modelos %>%
  select(centroid:vertical_density,
         Negative_index)

# Ajuste completo
doParallel::registerDoParallel()

ajuste_rf_completo <- flujo1 %>%
  finalize_workflow(best_model) %>%
  fit(df_explicacion)

doParallel::stopImplicitCluster()

# Nueva observación
nueva_observacion <-
  df_explicacion %>% 
  slice(7)

ajuste_rf_completo %>% 
  predict(new_data = nueva_observacion) %>% 
  mutate(real = nueva_observacion$Negative_index)
```


### Shapley

```{r}
# Explicador
prueba_rf <- explain_tidymodels(
  ajuste_rf_completo,
  data = df_explicacion %>% select(-c(Negative_index)),
  y = df_explicacion$Negative_index,
  verbose = FALSE
)



set.seed(2022)
shap_info <- 
  predict_parts(
    explainer = prueba_rf, 
    new_observation = nueva_observacion, 
    type = "shap",
    B = 100
  )

shap_info %>%
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(data = ~distinct(., variable, mean_val), 
           aes(mean_val, variable), 
           alpha = 0.5) +
  geom_boxplot(width = 0.5) +
  theme(legend.position = "none") +
  scale_fill_viridis_d() +
  labs(y = NULL)
```

### Importancia de variables

```{r}
set.seed(1803)
vip_model <- model_parts(prueba_rf)

vip_model %>% 
  tibble() %>%
  filter(!variable %in% c("_full_model_", "_baseline_")) %>% 
  group_by(variable) %>% 
  summarise(dropout_loss = mean(dropout_loss)) %>% 
  ggplot(aes(x = fct_reorder(variable, dropout_loss), y = dropout_loss)) +
  geom_col() +
  coord_flip() +
  labs(title = "Importancia de variables", x = "", y = "Importancia")
```

### PDP Amplitud

```{r}
# Función necesaria para los gráficos
ggplot_pdp <- function(obj, x) {
  
  p <- 
    as_tibble(obj$agr_profiles) %>%
    mutate(`_label_` = stringr::str_remove(`_label_`, "^[^_]*_")) %>%
    ggplot(aes(`_x_`, `_yhat_`)) +
    geom_line(data = as_tibble(obj$cp_profiles),
              aes(x = {{ x }}, group = `_ids_`),
              size = 0.5, alpha = 0.05, color = "gray50")
  
  num_colors <- n_distinct(obj$agr_profiles$`_label_`)
  
  if (num_colors > 1) {
    p <- p + geom_line(aes(color = `_label_`), size = 1.2, alpha = 0.8)
  } else {
    p <- p + geom_line(color = "midnightblue", size = 1.2, alpha = 0.8)
  }
  
  p
}

set.seed(2022)
pdp_amplitude <- model_profile(prueba_rf, N = 5000, variables = "amplitude")

ggplot_pdp(pdp_amplitude, amplitude)  +
  labs(x = "amplitude", 
       y = "IN") +
  scale_x_continuous(n.breaks = 10)

```

### PDP densidad vertical

```{r}
set.seed(2022)
pdp_vd <- model_profile(prueba_rf, N = 5000, variables = "vertical_density")

ggplot_pdp(pdp_vd, vertical_density)  +
  labs(x = "vertical_density", 
       y = "IP") +
  scale_x_continuous(n.breaks = 10)

```

### PDP BPM

```{r}
set.seed(2022)
pdp_bpm <- model_profile(prueba_rf, N = 5000, variables = "bpm")

ggplot_pdp(pdp_bpm, bpm)  +
  labs(x = "bpm", 
       y = "IP") +
  scale_x_continuous(n.breaks = 10)

```



